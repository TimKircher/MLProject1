{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "eaee7b63",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load -> clean -> features -> fit/validate -> predict -> submit\n",
    "from linear_model_base import RidgeRegression\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3cf25257",
   "metadata": {},
   "outputs": [],
   "source": [
    "from data_cleaner import Data_Cleaner\n",
    "\n",
    "#80/20 train test split to verify predictions before upload\n",
    "\n",
    "data_cleaner_train = Data_Cleaner(\"C:/Users/Tim/Documents/GitHub/MLProject1/project1/data/train.csv\")\n",
    "data_cleaner_train.tX = data_cleaner_train.tX[:200000,:]\n",
    "data_cleaner_train.y = data_cleaner_train.y[:200000]\n",
    "\n",
    "data_cleaner_train._fill_with_NaN()\n",
    "data_cleaner_train.replace_with_zero()\n",
    "minimum, maximum = data_cleaner_train.getMinMax()\n",
    "data_cleaner_train.standardize()\n",
    "\n",
    "\n",
    "data_cleaner_test = Data_Cleaner(\"C:/Users/Tim/Documents/GitHub/MLProject1/project1/data/train.csv\")\n",
    "data_cleaner_test.tX = data_cleaner_test.tX[200000:,:]\n",
    "data_cleaner_test.y = data_cleaner_test.y[200000:]\n",
    "\n",
    "data_cleaner_test._fill_with_NaN()\n",
    "data_cleaner_test.replace_with_zero()\n",
    "data_cleaner_test.tX = (data_cleaner_test.tX-minimum)/(maximum-minimum)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "15a0000c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_poly(x, degree, add_degree_zero=False):\n",
    "    \"\"\"polynomial basis functions for input data x, for j=0 up to j=degree.\"\"\"\n",
    "    # ***************************************************\n",
    "    # INSERT YOUR CODE HERE\n",
    "    # polynomial basis function: TODO\n",
    "    # this function should return the matrix formed\n",
    "    # by applying the polynomial basis to the input data\n",
    "    if add_degree_zero:\n",
    "        xN = np.hstack([np.ones([x.shape[0],1]),x])\n",
    "    else:\n",
    "        xN = x\n",
    "    if degree>0:\n",
    "        for i in range(degree-1):\n",
    "            xN = np.hstack([xN, x**(i+2)])\n",
    "    return np.array(xN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2c9ca583",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rho_compute(y,X,w,j):\n",
    "    #y is the response variable\n",
    "    #X is the predictor variables matrix\n",
    "    #w is the weight vector\n",
    "    #j is the feature selector\n",
    "    X_k = np.delete(X,j,1) #Remove the j variable i.e. j column\n",
    "    w_k = np.delete(w,j) #Remove the weight j\n",
    "    predict_k = predicted_values(X_k, w_k)\n",
    "    residual = y - predict_k\n",
    "    rho_j = np.sum(X[:,j]*residual)\n",
    "    return(rho_j)\n",
    "\n",
    "#z computation for unnormalised features\n",
    "def z_compute(X):\n",
    "    z_vector = np.sum(X*X, axis = 0) #Compute sum for each column\n",
    "    return(z_vector)\n",
    "\n",
    "def coordinate_descent(y,X,w,alpha,z,tolerance):\n",
    "    max_step = 100.\n",
    "    iteration = 0\n",
    "    while(max_step > tolerance):\n",
    "        iteration += 1\n",
    "        print(\"Iteration (start) : \",iteration)\n",
    "        old_weights = np.copy(w)\n",
    "        #print(\"\\nOld Weights\\n\",old_weights)\n",
    "        for j in range(len(w)): #Take the number of features ie columns\n",
    "            rho_j = rho_compute(y,X,w,j)\n",
    "            if j == 0: #Intercept is not included with the alpha regularisation\n",
    "                w[j] = rho_j/z[j]\n",
    "            elif rho_j < -alpha*len(y):\n",
    "                w[j] = (rho_j + (alpha*len(y)))/z[j]\n",
    "            elif rho_j > -alpha*len(y) and rho_j < alpha*len(y):\n",
    "                w[j] = 0.\n",
    "            elif rho_j > alpha*len(y):\n",
    "                w[j] = (rho_j - (alpha*len(y)))/z[j]\n",
    "            else:\n",
    "                w[j] = np.NaN\n",
    "            #print(\"step\"+ str(j))\n",
    "        #print(\"\\nNew Weights\\n\",w)\n",
    "        step_sizes = abs(old_weights - w)\n",
    "        #print(\"\\nStep sizes\\n\",step_sizes)\n",
    "        max_step = step_sizes.max()\n",
    "        #print(\"\\nMax step:\",max_step)\n",
    "        \n",
    "        \n",
    "    return(w, iteration, max_step)\n",
    "\n",
    "def predicted_values(X, w):\n",
    "    # X will be n x (d+1)\n",
    "    # w will be (d+1) x 1\n",
    "    predictions = np.matmul(X,w) # n x 1\n",
    "    return(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "48fa3759",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration (start) :  1\n",
      "Iteration (start) :  2\n",
      "Iteration (start) :  3\n",
      "Iteration (start) :  4\n",
      "Iteration (start) :  5\n",
      "Iteration (start) :  6\n",
      "Iteration (start) :  7\n",
      "Iteration (start) :  8\n",
      "Iteration (start) :  9\n",
      "Iteration (start) :  10\n"
     ]
    }
   ],
   "source": [
    "data_cleaner_train.tX = build_poly(data_cleaner_train.tX, 3, add_degree_zero=True)\n",
    "data_cleaner_test.tX = build_poly(data_cleaner_test.tX, 3, add_degree_zero=True)\n",
    "\n",
    "#Initialise weight/parameter vector, w, to be a zero vector\n",
    "w = np.zeros(data_cleaner_train.tX.shape[1], dtype = float)\n",
    "\n",
    "#Pre-compute the z_j term\n",
    "z = z_compute(data_cleaner_train.tX)\n",
    "\n",
    "#Set the alpha and tolerance level\n",
    "alpha = 0.001\n",
    "tolerance = 0.1\n",
    "\n",
    "w_opt, iterations, max_step = coordinate_descent(data_cleaner_train.y ,data_cleaner_train.tX ,w,alpha,z,tolerance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "cb10f3f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "weights = Model._run(lambda_ = best_lambda)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9a651c4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from proj1_helpers import *\n",
    "y_pred = predict_labels(w_opt, data_cleaner_test.tX)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "ed5b8bb3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5592479226305184\n"
     ]
    }
   ],
   "source": [
    "from costs import *\n",
    "err = compute_mse(data_cleaner_test.y, data_cleaner_test.tX, weights)\n",
    "print(np.sqrt(err))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "07010e38",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.73012"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def estimate_Leaderboard_score(y_true,weights,data):\n",
    "    \"\"\"Helper function estimating the categorical accuracy on the leaderscore\n",
    "    \"\"\"\n",
    "    y_pred = predict_labels(weights, data)\n",
    "    N_tot = y_pred.shape[0]\n",
    "    N_true = len(np.where(y_pred == y_true)[0])\n",
    "    categorical_acuracy = N_true/N_tot\n",
    "    return categorical_acuracy\n",
    "\n",
    "estimate_Leaderboard_score(data_cleaner_test.y, w_opt, data_cleaner_test.tX)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
