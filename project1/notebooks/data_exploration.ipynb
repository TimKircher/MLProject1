{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f8a9d08d-ada7-4858-b523-d03182c5338e",
   "metadata": {},
   "source": [
    "# Data Exploration\n",
    "\n",
    "#### In this notebook we will explore the dataset that we are going to train our models with. We will try to find out insides about each feature and about the relations between them\n",
    "\n",
    "We begin by importing the libraries that we are going to need for this procedure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd53fc9c-793a-4beb-8986-7365235bb55d",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import sys\n",
    "\n",
    "SCRIPTS_FILEPATH = \"./../scripts/\"\n",
    "DATA_FILEPATH = \"../data/train.csv\"\n",
    "\n",
    "sys.path.append(SCRIPTS_FILEPATH)\n",
    "from data_cleaner import Data_Cleaner\n",
    "from proj1_helpers import *\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48e0907d-45ff-4351-b802-2c1f93e49724",
   "metadata": {},
   "source": [
    "#### Loading the dataset\n",
    "The next step is to load our dataset. We do that using the Data_Cleaner class whose initialisation does the following : Load the dataset and creates different numpy arrays for the feature matrix, the labels and the ids. Additionaly, it creates a dictionary with the feature names as keys and their index number as value pairs. We also print the shape of each of the 3 parts of the dataset to get a graps of its dimensions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9304b343-a1f6-4d61-a9b1-c12b1db8e607",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = Data_Cleaner(DATA_FILEPATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6a3fae3-e122-40e9-a31d-ae8be5ccd94f",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.feature_names.items()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8caa27b6-b50e-44c9-8157-7b0e9dff2592",
   "metadata": {},
   "source": [
    "#### The dataset in a glimpse\n",
    "We start by taking an overall look of the dimensions of our dataset to get an idea about it. We also check about the percentage of each event/class to see if our training set is balanced."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae2950a4-fff6-42d5-a021-670fc38d74c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Shape of feature matrix : {}\".format(data.tX.shape))\n",
    "print(\"Shape of labels column : {}\".format(data.y.shape))\n",
    "print(\"Shape of ids column : {}\".format(data.ids.shape))\n",
    "\n",
    "signal_percentage = data.y[data.y == 1 ].sum() / len(data.y) * 100\n",
    "background_percentage = 100 - signal_percentage\n",
    "print(\"Percentage of signal events in dataset : {:.1f}\".format(signal_percentage))\n",
    "print(\"Percentage of background events in dataset : {:.1f}\".format(background_percentage))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3388b501-d8fd-4f70-8d89-f21068c077ad",
   "metadata": {},
   "source": [
    "#### Number of unique values, mean, histogram of values\n",
    "Moving on, we want to obtain some numerical measures about these features like their mean value and standard deviation as well as the percentage of the variables that are indicated as \"may be undefined\". From the description of the Higgs Challenge we know that these \"undefined\" variables are always equal to -999. So we use the function _fill_with_NaN() from our Data_Cleaner class to replace all -999 values with NaN. By doing that, we are able to calculate and print these numbers for all defined variables and finally plot 2 graphs summarizing them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32cb2426-ef03-45d2-bf0a-5227044485e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "data._fill_with_NaN()\n",
    "\n",
    "percentages = []\n",
    "mean_values = []\n",
    "std_values = []\n",
    "\n",
    "for feature_name, index in data.feature_names.items():\n",
    "    f_vals = data.tX[:,index]\n",
    "    num_unique = len(np.unique(f_vals))\n",
    "    print(\"{} has {} unique values\\n\".format(feature_name,num_unique))\n",
    "    \n",
    "    mean = np.nanmean(f_vals)\n",
    "    std = np.nanstd(f_vals)\n",
    "    percentage = np.count_nonzero(~np.isnan(f_vals))/len(f_vals)*100\n",
    "    mean_values.append(mean)\n",
    "    std_values.append(std)\n",
    "    percentages.append(percentage)    \n",
    "    \n",
    "    print(\"Percent (%) of samples that have entry: {:.1f}\".format(percentage))\n",
    "    print(\"Mean value: {:.3f}\".format(mean))\n",
    "    print(\"Standard deviation : {:.3f}\".format(std))\n",
    "    print(\"Ratio for std/mean: {:.3f}\".format(std/mean))\n",
    "    plt.figure()\n",
    "    plt.hist(f_vals,bins= 100)\n",
    "    plt.xlabel(\"{} distribution\".format(feature_name))\n",
    "    plt.ylabel('Frequency')\n",
    "    plt.savefig('../plots/Distribution for {}'.format(feature_name))\n",
    "    plt.show()\n",
    "    print(\"--------------------------------\\n\")\n",
    "\n",
    "plt.figure()\n",
    "plt.scatter(mean_values, std_values)  \n",
    "plt.xlabel('Mean value')\n",
    "plt.ylabel('Standard deviation')\n",
    "plt.savefig('../plots/mean_vs_std2.png')\n",
    "\n",
    "plt.show\n",
    "\n",
    "plt.figure(figsize=(8,4))\n",
    "plt.plot(np.linspace(1,30,num = 30),percentages)\n",
    "plt.xticks(np.linspace(1,30,num = 30))\n",
    "plt.xlabel('Feature')\n",
    "plt.ylabel('Percentage %')\n",
    "plt.grid()\n",
    "plt.savefig('../plots/Percentage')\n",
    "plt.show"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cc147b5-4a51-4212-a663-25c76750051b",
   "metadata": {},
   "source": [
    "#### Cleaning the dataset \n",
    "For the following procedures the np.NaN values pose a problem that we have to solve. We identify 2 cases for variables that are indicated as \"may be undefined\" :\n",
    "1. Variables from the first feature DER_mass_MMC which estimates the mass $m_H$ of the Higgs boson candidate. In this case these variables can't be defined as the topology of the event is too far from the expected topology. To solve this, we fill all np.NaN values of this feature with the median value of the remaining feature's values.\n",
    "2. Features related to jets. These features have some of their variables undefined because jets have not been detected in the event and so they are meaningless. To tackle this problem we fill all these undefined variables with 0.\n",
    "\n",
    "After treating all the undefined variables we also normalize our dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc61f9a3-9691-4e0b-ac22-25086aa6b489",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.fix_mass_MMC(add_impute_array = False)\n",
    "data.replace_with_zero()\n",
    "data.normalize()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7dfbd42e-1233-4c4b-84e6-bb3ff2b0704a",
   "metadata": {},
   "source": [
    "#### Correlation of features\n",
    "We visualize the correlation between the features of our dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e276d95-953f-4863-8b40-ca72be43c294",
   "metadata": {},
   "outputs": [],
   "source": [
    "corr_mat = np.corrcoef(data.tX,rowvar=False)\n",
    "plt.matshow(corr_mat)\n",
    "plt.colorbar()\n",
    "plt.savefig('../plots/Correlation')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "025a2908-3e13-48fb-bcfc-34694a35bab2",
   "metadata": {},
   "source": [
    "#### Principal component analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df5b9d45-a3e6-47b6-8e7b-96e45ca84b96",
   "metadata": {},
   "outputs": [],
   "source": [
    "cov_mat = np.cov(data.tX.T) #calculate covariance matrix\n",
    "eigval_pca, eigvec_pca = np.linalg.eig(cov_mat) #can not be orderd, but they are here\n",
    "\n",
    "total_eigval = np.sum(eigval_pca)\n",
    "percentages = [eigval/total_eigval for eigval in eigval_pca]\n",
    "percentages_cumulative = np.cumsum(percentages)\n",
    "plt.plot(np.arange(1,len(eigval_pca)+1),percentages_cumulative)\n",
    "plt.xlim(1,len(eigval_pca))\n",
    "plt.ylabel(\"Total variance \\\"explained\\\" \")\n",
    "plt.xlabel(\"no. principal component\")\n",
    "plt.savefig('../plots/PCA')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a2e4015-3a0e-490e-91e4-1c4e0856e44d",
   "metadata": {},
   "source": [
    "Arbitrarily choose cutoff when more than $0.95$% of the cumulative variance is explained"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09ad1aef-a6b2-489c-b389-edeefd37f49c",
   "metadata": {},
   "outputs": [],
   "source": [
    "greater_095 = np.argmax(percentages_cumulative > 0.95) #stops at first true\n",
    "print(\"{} principal components can explain more than 95% of the variance\".format(greater_095+1))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4da4c224-4ad1-4a13-a5ff-6a53f4f4d4da",
   "metadata": {},
   "source": [
    "#### Project onto principal components\n",
    "v : (..., M, M) array \n",
    "  The normalized (unit \"length\") eigenvectors, \n",
    "  such that the \n",
    "        column ``v[:,i]`` is the eigenvector corresponding to the \n",
    "        eigenvalue ``w[i]``."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed0ffd4b-3beb-48a8-ac7c-f721ca0598af",
   "metadata": {},
   "outputs": [],
   "source": [
    "pcas_095 = eigvec_pca[:,:greater_095]\n",
    "pcas_095_other = (eigvec_pca.T[:][:greater_095]).T\n",
    "np.allclose(pcas_095_other, pcas_095)\n",
    "projection_mat = eigvec_pca[:,:greater_095]\n",
    "projected_data = data.tX @  projection_mat\n",
    "projected_data.shape"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
